{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318dc832",
   "metadata": {},
   "source": [
    "# Are you using the right version of python??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "required = \"3.11.10\"\n",
    "if not sys.version.startswith(required):\n",
    "    raise RuntimeError(\n",
    "        f\"❌ Wrong Python version.\\n\"\n",
    "        f\"You are using: {sys.version}\\n\"\n",
    "        f\"This project requires: Python {required}.\\n\"\n",
    "    )\n",
    "\n",
    "print(\"✅ Python version OK:\", sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febde5ba",
   "metadata": {},
   "source": [
    "# Cell 1 - Imports & Data registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "295ead1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Tuple, Type, Dict, Any\n",
    "\n",
    "from adapters.base import DatasetAdapter\n",
    "from adapters.cybermetric import CyberMetricAdapter\n",
    "from adapters.cipherbank import CipherBankAdapter\n",
    "from adapters.cipherbench import CipherBenchAdapter\n",
    "from models.base import get_model\n",
    "\n",
    "# Dataset registry: name -> (AdapterClass, path)\n",
    "DATASETS = {\n",
    "    \"cybermetric\": (\n",
    "        CyberMetricAdapter,\n",
    "        os.path.join(\"datasets\", \"cybermetric\", \"cybermetric.json\"),\n",
    "    ),\n",
    "    \"cipherbank\": (\n",
    "        CipherBankAdapter,\n",
    "        os.path.join(\"datasets\", \"cipherbank\", \"cipherbank.jsonl\"),\n",
    "    ),\n",
    "    \"cipherbench\": (\n",
    "        CipherBenchAdapter,\n",
    "        os.path.join(\"datasets\", \"cipherbench\", \"cipherbench.jsonl\"),\n",
    "    ),\n",
    "}\n",
    "\n",
    "GEN_CONFIG = {\n",
    "    \"cipherbench\": {\n",
    "        \"max_output_tokens\": 1536,  \n",
    "    },\n",
    "    \"cipherbank\": {\n",
    "        \"max_output_tokens\": 1536,\n",
    "    },\n",
    "    \"cybermetric\": {\n",
    "        \"max_output_tokens\": 128,\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015cbce3",
   "metadata": {},
   "source": [
    "# Cell 2 - Helpers & constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1b2e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_output(raw: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize model output into a status + text payload without throwing.\n",
    "    (Kept for compatibility if you want to use it later.)\n",
    "    \"\"\"\n",
    "    if not isinstance(raw, str):\n",
    "        return {\"status\": \"error\", \"output\": \"\", \"error_message\": f\"Non-string output: {type(raw)}\"}\n",
    "    s = raw.strip()\n",
    "    if s.startswith(\"[Rate-limited\"):\n",
    "        return {\"status\": \"rate_limited\", \"output\": \"\"}\n",
    "    if s.startswith(\"[Empty content]\") or s.startswith(\"[Unexpected format]\"):\n",
    "        return {\"status\": \"empty\", \"output\": \"\"}\n",
    "    return {\"status\": \"ok\", \"output\": s}\n",
    "\n",
    "\n",
    "def _is_blank(s: str) -> bool:\n",
    "    return not isinstance(s, str) or not s.strip()\n",
    "\n",
    "\n",
    "CONTENT_RETRIES = int(os.getenv(\"EVAL_CONTENT_RETRIES\", \"2\"))\n",
    "CONTENT_BACKOFF = float(os.getenv(\"EVAL_CONTENT_BACKOFF\", \"1.5\"))  # seconds\n",
    "ANSWER_CUE = \"\\n\\nPlaintext:\"  # nudges models that return whitespace\n",
    "FALLBACK_OUTPUT = \"[NO_ANSWER]\"  # ensures every record has an output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d71cd",
   "metadata": {},
   "source": [
    "# Cell 3 - safe_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a64ec7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_generate(model, prompt: str, **gen_kwargs) -> dict:\n",
    "    attempts = 0\n",
    "    used_cue = False\n",
    "\n",
    "    # attempt 0: original prompt\n",
    "    attempts += 1\n",
    "    try:\n",
    "        out = model.generate(prompt, **gen_kwargs)\n",
    "        if _is_blank(out) or out.startswith(\"[Rate-limited\"):\n",
    "            raise ValueError(\"blank_or_rate_limited\")\n",
    "        return {\"status\": \"ok\", \"output\": out.strip(), \"attempts\": attempts, \"used_cue\": used_cue}\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        last_err = str(e)\n",
    "\n",
    "    # retries with cue\n",
    "    retry_prompt = prompt + ANSWER_CUE\n",
    "    used_cue = True\n",
    "    for _ in range(CONTENT_RETRIES):\n",
    "        time.sleep(CONTENT_BACKOFF)\n",
    "        attempts += 1\n",
    "        try:\n",
    "            out = model.generate(retry_prompt, **gen_kwargs)\n",
    "            if _is_blank(out) or out.startswith(\"[Rate-limited\"):\n",
    "                raise ValueError(\"blank_or_rate_limited\")\n",
    "            return {\"status\": \"ok\", \"output\": out.strip(), \"attempts\": attempts, \"used_cue\": used_cue}\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            last_err = str(e)\n",
    "\n",
    "    return {\n",
    "        \"status\": \"empty\",\n",
    "        \"output\": FALLBACK_OUTPUT,\n",
    "        \"error_message\": last_err,\n",
    "        \"attempts\": attempts,\n",
    "        \"used_cue\": used_cue,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d062bc",
   "metadata": {},
   "source": [
    "# Cell 4 - Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95afd265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIG – EDIT THESE BETWEEN RUNS ====\n",
    "\n",
    "DATASET_NAME = \"cybermetric\" # cipherbank, cipherbench, cybermetric\n",
    "MODEL_NAME   = \"mistral-7b-instruct\" #gemini-2.5-flash-lite, gpt-oss-20b, llama-3.3-70b-instruct, mistral-7b-instruct\n",
    "\n",
    "LIMIT = 6  # to stay under 1000 RPD\n",
    "\n",
    "START_INDEX = None  # <-- this is new: None = auto-resume\n",
    "\n",
    "OUT_PATH = None\n",
    "\n",
    "PROGRESS_EVERY = 25\n",
    "PER_ITEM_SLEEP = float(os.getenv(\"EVAL_MIN_SECS_BETWEEN_CALLS\", \"0\"))\n",
    "\n",
    "DATASET_SIZES = {\n",
    "    \"cipherbank\": 2358,\n",
    "    \"cipherbench\": 2400,\n",
    "    \"cybermetric\": 1500,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48ba86",
   "metadata": {},
   "source": [
    "# Cell 5 - infer_start_index() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e2a1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_start_index(dataset_name: str, model_name: str, out_path: str | None = None) -> int:\n",
    "    \"\"\"\n",
    "    If a results file already exists, return the number of lines in it.\n",
    "    That corresponds to how many items you've already processed.\n",
    "    \"\"\"\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(\"results\", f\"{dataset_name}__{model_name}.jsonl\")\n",
    "    if not os.path.exists(out_path):\n",
    "        return 0\n",
    "    count = 0\n",
    "    with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for _ in f:\n",
    "            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e71a59",
   "metadata": {},
   "source": [
    "# Cell 6 - Planning cell (shows what will run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1021c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cybermetric\n",
      "Model:   mistral-7b-instruct\n",
      "Already done (in results file): 0\n",
      "Total size: 1500\n",
      "Remaining after this start index: 1500\n",
      "Will process this run (LIMIT): 6\n",
      "Remaining AFTER this run: 1494\n",
      "\n",
      "If this looks wrong, adjust START_INDEX or LIMIT before running eval.\n"
     ]
    }
   ],
   "source": [
    "# Decide START_INDEX\n",
    "if START_INDEX is None:\n",
    "    START_INDEX = infer_start_index(DATASET_NAME, MODEL_NAME, OUT_PATH)\n",
    "\n",
    "total_size = DATASET_SIZES.get(DATASET_NAME, None)\n",
    "already_done = START_INDEX\n",
    "remaining = None if total_size is None else max(total_size - already_done, 0)\n",
    "\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Model:   {MODEL_NAME}\")\n",
    "print(f\"Already done (in results file): {already_done}\")\n",
    "\n",
    "if total_size is not None:\n",
    "    print(f\"Total size: {total_size}\")\n",
    "    print(f\"Remaining after this start index: {remaining}\")\n",
    "    if LIMIT is not None:\n",
    "        will_process = min(LIMIT, remaining)\n",
    "        print(f\"Will process this run (LIMIT): {will_process}\")\n",
    "        print(f\"Remaining AFTER this run: {remaining - will_process}\")\n",
    "else:\n",
    "    print(\"(No dataset size info; only showing already_done.)\")\n",
    "\n",
    "print(\"\\nIf this looks wrong, adjust START_INDEX or LIMIT before running eval.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e4137",
   "metadata": {},
   "source": [
    "# Cell 7 - run_eval() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "723eae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(\n",
    "    dataset_name: str,\n",
    "    model_name: str,\n",
    "    limit: int | None = None,\n",
    "    start_index: int = 0,\n",
    "    out_path: str | None = None,\n",
    "    progress_every: int = 25,\n",
    "    per_item_sleep: float = 0.0,\n",
    "):\n",
    "    # Load dataset adapter\n",
    "    if dataset_name not in DATASETS:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "    AdapterCls, ds_path = DATASETS[dataset_name]\n",
    "    adapter: DatasetAdapter = AdapterCls(ds_path)\n",
    "\n",
    "    # Load model\n",
    "    model = get_model(model_name)\n",
    "\n",
    "    # Prepare output\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    out_path = out_path or os.path.join(\"results\", f\"{dataset_name}__{model.name}.jsonl\")\n",
    "\n",
    "    # If resuming (start_index > 0) and file exists, append instead of overwrite\n",
    "    mode = \"a\" if start_index > 0 and os.path.exists(out_path) else \"w\"\n",
    "    if mode == \"a\":\n",
    "        print(f\"Appending to existing file: {out_path} (starting at dataset index {start_index})\")\n",
    "    else:\n",
    "        print(f\"Writing new results file: {out_path}\")\n",
    "\n",
    "    correct = 0\n",
    "    processed = 0  # number of items actually evaluated in THIS run\n",
    "\n",
    "    try:\n",
    "        with open(out_path, mode, encoding=\"utf-8\") as fout:\n",
    "            for ds_idx, item in enumerate(adapter.iter_items()):\n",
    "                # Skip until we reach START_INDEX\n",
    "                if ds_idx < start_index:\n",
    "                    continue\n",
    "\n",
    "                # Enforce session limit (relative to start_index)\n",
    "                if limit is not None and processed >= limit:\n",
    "                    break\n",
    "\n",
    "                prompt = adapter.build_prompt(item)\n",
    "\n",
    "                # Call model safely\n",
    "                gen_cfg = GEN_CONFIG.get(dataset_name, {})\n",
    "                result = safe_generate(model, prompt, **gen_cfg)\n",
    "                status = result[\"status\"]\n",
    "                output = result.get(\"output\", \"\")\n",
    "                error_message = result.get(\"error_message\", \"\")\n",
    "                attempts = result.get(\"attempts\", 1)\n",
    "                used_cue = result.get(\"used_cue\", False)\n",
    "\n",
    "                # Only score \"ok\" outputs; others count as incorrect\n",
    "                if status == \"ok\":\n",
    "                    label = adapter.score(item, output)\n",
    "                else:\n",
    "                    label = 0\n",
    "\n",
    "                correct += label\n",
    "                processed += 1\n",
    "\n",
    "                rec = {\n",
    "                    \"id\": item.get(\"id\", ds_idx),\n",
    "                    \"dataset\": dataset_name,\n",
    "                    \"model\": model.name,\n",
    "                    \"status\": status,          # ok | rate_limited | empty | error\n",
    "                    \"prompt\": prompt,\n",
    "                    \"output\": output,\n",
    "                    \"correct\": label,\n",
    "                    \"attempts\": attempts,\n",
    "                    \"used_cue\": used_cue,\n",
    "                }\n",
    "                if error_message:\n",
    "                    rec[\"error_message\"] = error_message\n",
    "\n",
    "                # Include extra metadata if present\n",
    "                for key in (\"algorithm\", \"ciphertext\", \"prompt_text\", \"question\"):\n",
    "                    if key in item:\n",
    "                        rec[key] = item[key]\n",
    "\n",
    "                fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "                # Optional progress logging\n",
    "                if progress_every and processed % progress_every == 0:\n",
    "                    acc_so_far = correct / processed if processed else 0.0\n",
    "                    print(\n",
    "                        f\"[processed={processed} | ds_idx={ds_idx}] \"\n",
    "                        f\"running acc={acc_so_far:.4f} (last status={status})\"\n",
    "                    )\n",
    "\n",
    "                # Optional gentle pacing (useful for free-tier models)\n",
    "                if per_item_sleep > 0:\n",
    "                    time.sleep(per_item_sleep)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted by user. Finalizing...\")\n",
    "\n",
    "    acc = correct / processed if processed else 0.0\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Model:   {model.name}\")\n",
    "    print(f\"Start index: {start_index}\")\n",
    "    print(f\"Processed this run: {processed}\")\n",
    "    print(f\"Accuracy this run: {acc:.4f}\")\n",
    "    print(f\"Wrote results to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6ebcd",
   "metadata": {},
   "source": [
    "# Cell 7 - Run cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ee4c0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing new results file: results/cybermetric__mistral-7b-instruct.jsonl\n",
      "------------------------------------------------------------\n",
      "Dataset: cybermetric\n",
      "Model:   mistral-7b-instruct\n",
      "Start index: 0\n",
      "Processed this run: 6\n",
      "Accuracy this run: 1.0000\n",
      "Wrote results to: results/cybermetric__mistral-7b-instruct.jsonl\n"
     ]
    }
   ],
   "source": [
    "run_eval(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    model_name=MODEL_NAME,\n",
    "    limit=LIMIT,\n",
    "    start_index=START_INDEX,\n",
    "    out_path=OUT_PATH,\n",
    "    progress_every=PROGRESS_EVERY,\n",
    "    per_item_sleep=PER_ITEM_SLEEP,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
