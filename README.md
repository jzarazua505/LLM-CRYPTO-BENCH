# ðŸ§  LLM-CRYPTO-BENCH

Benchmarking framework for evaluating Large Language Models (LLMs) on cryptography-related reasoning and problem-solving tasks.

---

## âš™ï¸ Project Overview

This repository provides a unified interface for evaluating multiple LLMs (Gemini, LLaMA, Mixtral, etc.) across three standardized cryptography datasets:

- **CipherBank** â€“ structured CTF-style tasks from Hugging Face.
- **CipherBench** â€“ question/answerâ€“based cipher tasks.
- **CyberMetric** â€“ curated real-world multiple-choice questions.

Each dataset is processed through adapter modules, evaluated via model backends, and scored using uniform metrics.

---

## ðŸ§© Requirements

**Python Version (Important):**
> âœ… Use **Python 3.11.10** only.

All dependencies are verified with Python 3.11.10 to prevent incompatibilities with the new `google-genai` SDK and `pandas` release.

Check your version:

```bash
python3 --version
